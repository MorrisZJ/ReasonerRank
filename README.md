# ReasonerRank

**ACL 2025 Findings** â€” *ReasonerRank: Redefining Language Model Evaluation with Ground-Truth-Free Ranking Frameworks*  
**Keywords**: Evaluation Â· Large Language Models Â· Reasoning Â· Ground-Truth-Free

## âœ¨ TL;DR

We propose **ReasonerRank**, a **ground-truth-free evaluation framework** for ranking LLMs. It focuses on **reasoning consistency** and **instruction adherence**, outperforming existing approaches in complex, label-scarce settings.

## ðŸ§  Abstract

Conventional LLM evaluation depends on ground-truth labelsâ€”often unavailable or costly. **ReasonerRank** shifts the focus to evaluating the **transparency and coherence of reasoning**, not correctness alone.  
Each LLM response is segmented into:

- ðŸŸ© **Direct answer**  
- ðŸ§© **Multi-step explanation**  
- ðŸ“š **Supporting evidence**

We introduce **TopK-ReRank**, which builds a consensus answer from top models to reduce ambiguity across diverse reasoning styles.  
ReasonerRank outperforms majority voting, triplet ranking, and peer-review methods on multiple tasks.

---

## âœ… TODO List

### ðŸ“„ Docs & Release
- [x] Draft README
- [ ] Add usage guide

### ðŸ§© Baselines & Method Code Release
- [ ] Majority Voting
- [ ] Triplet Ranking
- [ ] LLM-as-a-Judge
- [ ] ReasonerRank (ours)
- [ ] End-to-end eval pipeline script

### ðŸ“Š Experiments
- [ ] TopK size ablation
- [ ] Low-agreement / adversarial cases
- [ ] Generalization to new tasks
